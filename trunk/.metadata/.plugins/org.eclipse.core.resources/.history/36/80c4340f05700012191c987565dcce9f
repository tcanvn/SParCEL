/**
 * Copyright (C) 2007-2008, Jens Lehmann
 *
 * This file is part of DL-Learner.
 * 
 * DL-Learner is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 3 of the License, or
 * (at your option) any later version.
 *
 * DL-Learner is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *
 */
package org.dllearner.cli.ParCEL;

import java.io.File;
import java.text.DecimalFormat;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.List;
import java.util.Random;
import java.util.Set;
import java.util.TreeSet;

import org.apache.log4j.Logger;
import org.dllearner.algorithms.celoe.CELOE;
import org.dllearner.cli.CrossValidation;
import org.dllearner.cli.ParCEL.ParCELExFortifiedCrossValidation.URIComparator;
import org.dllearner.core.ComponentInitException;
import org.dllearner.core.AbstractCELA;
import org.dllearner.core.AbstractReasonerComponent;
import org.dllearner.core.owl.Description;
import org.dllearner.core.owl.Individual;
import org.dllearner.kb.OWLFile;
import org.dllearner.learningproblems.Heuristics;
import org.dllearner.learningproblems.PosNegLP;
import org.dllearner.utilities.Helper;
import org.dllearner.utilities.datastructures.Datastructures;
import org.dllearner.utilities.owl.ConceptComparator;
import org.dllearner.utilities.owl.OWLAPIConverter;
import org.dllearner.utilities.statistics.Stat;
import org.dllearner.utilities.Files;
import org.semanticweb.owlapi.apibinding.OWLManager;
import org.semanticweb.owlapi.model.OWLClassExpression;
import org.semanticweb.owlapi.model.OWLOntology;
import org.semanticweb.owlapi.model.OWLOntologyManager;

import com.clarkparsia.pellet.owlapiv3.PelletReasoner;
import com.clarkparsia.pellet.owlapiv3.PelletReasonerFactory;

/**
 * Performs cross validation for the given problem. Supports k-fold cross-validation and
 * leave-one-out cross-validation.
 * 
 * @author Jens Lehmann
 * 
 */
public class CELOEFortifiedCrossValidation extends CrossValidation {
	

	protected Stat noOfCounterPartialDefinitions;
	protected Stat noOfCounterPartialDefinitionUsed;
	protected Stat avgCounterPartialDefinitionLength;
	protected Stat avgCounterPartialDefinitionCoverage;
	
	//fortify variables
	protected Stat fortifiedDefinitionLengthStat;
	protected Stat fortifyDefinitionLengthStat;
	
	protected Stat accuracyFortifyStat;
	protected Stat correctnessFortifyStat;
	protected Stat completenessFortifyStat;
	protected Stat fmeasureFortifyStat;
	
	protected Stat avgFortifyCoverageTraingStat;
	protected Stat avgFortifyCoverageTestStat;
	
	protected Stat fortifiedRuntime;
	
	//---------
	Logger logger = Logger.getLogger(this.getClass());

	public CELOEFortifiedCrossValidation() {

	}
	
	public CELOEFortifiedCrossValidation(AbstractCELA la, PosNegLP lp, AbstractReasonerComponent rs, int folds,
			boolean leaveOneOut) {

		this(la, lp, rs, folds, leaveOneOut, 1);

	}

	public CELOEFortifiedCrossValidation(AbstractCELA la, PosNegLP lp, AbstractReasonerComponent rs, int folds,
			boolean leaveOneOut, int noOfRuns) {

		DecimalFormat df = new DecimalFormat();

		// the training and test sets used later on
		List<Set<Individual>> trainingSetsPos = new LinkedList<Set<Individual>>();
		List<Set<Individual>> trainingSetsNeg = new LinkedList<Set<Individual>>();
		List<Set<Individual>> testSetsPos = new LinkedList<Set<Individual>>();
		List<Set<Individual>> testSetsNeg = new LinkedList<Set<Individual>>();

		// get examples and shuffle them too
		Set<Individual> posExamples = ((PosNegLP) lp).getPositiveExamples();
		List<Individual> posExamplesList = new LinkedList<Individual>(posExamples);
		Collections.shuffle(posExamplesList, new Random(1));
		Set<Individual> negExamples = ((PosNegLP) lp).getNegativeExamples();
		List<Individual> negExamplesList = new LinkedList<Individual>(negExamples);
		Collections.shuffle(negExamplesList, new Random(2));

		// sanity check whether nr. of folds makes sense for this benchmark
		if (!leaveOneOut && (posExamples.size() < folds && negExamples.size() < folds)) {
			System.out.println("The number of folds is higher than the number of "
					+ "positive/negative examples. This can result in empty test sets. Exiting.");
			System.exit(0);
		}

		// calculating where to split the sets, ; note that we split
		// positive and negative examples separately such that the
		// distribution of positive and negative examples remains similar
		// (note that there are better but more complex ways to implement this,
		// which guarantee that the sum of the elements of a fold for pos
		// and neg differs by at most 1 - it can differ by 2 in our implementation,
		// e.g. with 3 folds, 4 pos. examples, 4 neg. examples)
		int[] splitsPos = calculateSplits(posExamples.size(), folds);
		int[] splitsNeg = calculateSplits(negExamples.size(), folds);
		
		long orthAllCheckCount[] = new long[5];
		orthAllCheckCount[0] = orthAllCheckCount[1] = orthAllCheckCount[2] = orthAllCheckCount[3] = orthAllCheckCount[4] = 0;
			
		long orthSelectedCheckCount[] = new long[5];
		orthSelectedCheckCount[0] = orthSelectedCheckCount[1] = orthSelectedCheckCount[2] = orthSelectedCheckCount[3] = orthSelectedCheckCount[4] = 0;

		// System.out.println(splitsPos[0]);
		// System.out.println(splitsNeg[0]);

		// calculating training and test sets
		for (int i = 0; i < folds; i++) {
			Set<Individual> testPos = getTestingSet(posExamplesList, splitsPos, i);
			Set<Individual> testNeg = getTestingSet(negExamplesList, splitsNeg, i);
			testSetsPos.add(i, testPos);
			testSetsNeg.add(i, testNeg);
			trainingSetsPos.add(i, getTrainingSet(posExamples, testPos));
			trainingSetsNeg.add(i, getTrainingSet(negExamples, testNeg));
		}

		// ---------------------------------
		// k-fold cross validation
		// ---------------------------------

		Stat runtimeAvg = new Stat();
		Stat runtimeMax = new Stat();
		Stat runtimeMin = new Stat();
		Stat runtimeDev = new Stat();

		Stat defLenAvg = new Stat();
		Stat defLenDev = new Stat();
		Stat defLenMax = new Stat();
		Stat defLenMin = new Stat();

		Stat trainingAccAvg = new Stat();
		Stat trainingAccDev = new Stat();
		Stat trainingAccMax = new Stat();
		Stat trainingAccMin = new Stat();

		Stat trainingCorAvg = new Stat();
		Stat trainingCorDev = new Stat();
		Stat trainingCorMax = new Stat();
		Stat trainingCorMin = new Stat();

		Stat trainingComAvg = new Stat();
		Stat trainingComDev = new Stat();
		Stat trainingComMax = new Stat();
		Stat trainingComMin = new Stat();

		Stat testingAccAvg = new Stat();
		Stat testingAccMax = new Stat();
		Stat testingAccMin = new Stat();
		Stat testingAccDev = new Stat();

		Stat testingCorAvg = new Stat();
		Stat testingCorDev = new Stat();
		Stat testingCorMax = new Stat();
		Stat testingCorMin = new Stat();

		Stat testingComAvg = new Stat();
		Stat testingComDev = new Stat();
		Stat testingComMax = new Stat();
		Stat testingComMin = new Stat();
		
		Stat testingFMesureAvg = new Stat();
		Stat testingFMesureDev = new Stat();
		Stat testingFMesureMax = new Stat();
		Stat testingFMesureMin = new Stat();
		
		Stat trainingFMesureAvg = new Stat();
		Stat trainingFMesureDev = new Stat();
		Stat trainingFMesureMax = new Stat();
		Stat trainingFMesureMin = new Stat();
		
		Stat noOfDescriptionsAgv = new Stat();
		Stat noOfDescriptionsMax = new Stat();
		Stat noOfDescriptionsMin = new Stat();
		Stat noOfDescriptionsDev = new Stat();
		
		//fortify stat. variables
		Stat noOfCounterPartialDefinitionsAvg = new Stat();
		Stat noOfCounterPartialDefinitionsDev = new Stat();
		Stat noOfCounterPartialDefinitionsMax = new Stat();
		Stat noOfCounterPartialDefinitionsMin = new Stat();
		
		Stat noOfCounterPartialDefinitionsUsedAvg = new Stat();
		Stat noOfCounterPartialDefinitionsUsedDev = new Stat();
		Stat noOfCounterPartialDefinitionsUsedMax = new Stat();
		Stat noOfCounterPartialDefinitionsUsedMin = new Stat();
		
		Stat avgCounterPartialDefinitionLengthAvg = new Stat();
		Stat avgCounterPartialDefinitionLengthDev = new Stat();
		Stat avgCounterPartialDefinitionLengthMax = new Stat();
		Stat avgCounterPartialDefinitionLengthMin = new Stat();
		
		
		Stat avgFirtifiedDefinitionLengthAvg = new Stat();
		Stat avgFirtifiedDefinitionLengthDev = new Stat();
		Stat avgFirtifiedDefinitionLengthMax = new Stat();
		Stat avgFirtifiedDefinitionLengthMin = new Stat();
		
		Stat accuracyFortifyAvg = new Stat();
		Stat accuracyFortifyDev = new Stat();
		Stat accuracyFortifyMax = new Stat();
		Stat accuracyFortifyMin = new Stat();
		
		Stat correctnessFortifyAvg = new Stat();
		Stat correctnessFortifyDev = new Stat();
		Stat correctnessFortifyMax = new Stat();
		Stat correctnessFortifyMin = new Stat();
		
		Stat completenessFortifyAvg = new Stat();
		Stat completenessFortifyDev = new Stat();
		Stat completenessFortifyMax = new Stat();
		Stat completenessFortifyMin = new Stat();		
		
		Stat fmeasureFortifyAvg = new Stat();
		Stat fmeasureFortifyDev = new Stat();
		Stat fmeasureFortifyMax = new Stat();
		Stat fmeasureFortifyMin = new Stat();
		
		Stat avgFortifyCoverageTraingAvg = new Stat();
		Stat avgFortifyCoverageTraingDev = new Stat();
		Stat avgFortifyCoverageTraingMax = new Stat();
		Stat avgFortifyCoverageTraingMin = new Stat();
		
		Stat avgFortifyCoverageTestAvg = new Stat();	
		Stat avgFortifyCoverageTestDev = new Stat();
		Stat avgFortifyCoverageTestMax = new Stat();
		Stat avgFortifyCoverageTestMin = new Stat();
		

		long ontologyLoadStarttime = System.nanoTime();		
		OWLOntologyManager manager = OWLManager.createOWLOntologyManager();
		OWLOntology ontology = ((OWLFile)la.getReasoner().getSources().iterator().next()).createOWLOntology(manager);			
		outputWriter("Ontology created, axiom count: " + ontology.getAxiomCount());
		PelletReasoner pelletReasoner = PelletReasonerFactory.getInstance().createReasoner(ontology);
		outputWriter("Pellet creared and binded with the ontology: " + pelletReasoner.getReasonerName());
		long ontologyLoadDuration = System.nanoTime() - ontologyLoadStarttime;
		outputWriter("Total time for creating and binding ontology: " + ontologyLoadDuration/1000000000d + "ms");

		
		for (int kk = 0; kk < noOfRuns; kk++) {

			//stat. variables for each fold ==> need to be re-created after each fold
			runtime = new Stat();
			length = new Stat();
			accuracyTraining = new Stat();
			trainingCorrectnessStat = new Stat();
			trainingCompletenessStat = new Stat();
			accuracy = new Stat();
			testingCorrectnessStat = new Stat();
			testingCompletenessStat = new Stat();
			fMeasure = new Stat();
			fMeasureTraining = new Stat();
			
			noOfCounterPartialDefinitions = new Stat();
			noOfCounterPartialDefinitionUsed = new Stat();
			avgCounterPartialDefinitionLength = new Stat();
			avgCounterPartialDefinitionCoverage = new Stat();
			
			totalNumberOfDescriptions = new Stat();
			
			//fortify variables
			fortifiedDefinitionLengthStat = new Stat();
			fortifyDefinitionLengthStat = new Stat();
			
			accuracyFortifyStat = new Stat();
			correctnessFortifyStat = new Stat();
			completenessFortifyStat = new Stat();
			fmeasureFortifyStat = new Stat();
			
			avgFortifyCoverageTraingStat = new Stat();
			avgFortifyCoverageTestStat = new Stat();	
			
			fortifiedRuntime = new Stat();

			// run the algorithm
			for (int currFold = 0; currFold < folds; currFold++) {
				
				outputWriter("//---------------\n" + "// Fold " + currFold + "/" + folds + "\n//---------------");
				
				//1. reserve the pos/neg examples and start the learner to get the counter partial definitions
				//2. store the counter partial definitions
				//3. reserve the pos/neg back to the original set and start the learner again to get the definition
				//4. do the test step and apply the fortification if necessary 

				
				//-----------------------------------------------------
				//1. reserve the pos/neg and let the learner starts  
				//----------------------------------------------------- 
				lp.setNegativeExamples(trainingSetsPos.get(currFold));
				lp.setPositiveExamples(trainingSetsNeg.get(currFold));

				try {
					lp.init();
					la.init();
				} catch (ComponentInitException e) {
					e.printStackTrace();
				}
				
				double orgNoise = ((CELOE)la).getNoisePercentage();
				
				outputWriter("** Phase 1 - Learning counter partial definition");
				outputWriter("Noise: 98%");
				
				((CELOE)la).setNoisePercentage(98);

				long algorithmStartTime1 = System.nanoTime();
				la.start();
				long algorithmDuration1 = System.nanoTime() - algorithmStartTime1;
								
				//2. store the counter partial definitions
				TreeSet<CELOE.PartialDefinition> counterPartialDefinitions = new TreeSet<CELOE.PartialDefinition>(new CoverageComparator()); 
				
				counterPartialDefinitions.addAll(((CELOE)la).getPartialDefinitions());			
				
				outputWriter("Finish learning, number of partial definitions: " + counterPartialDefinitions.size());
				outputWriter("CPDEF length and coverage (length, coverage): ");
				int count=1;
				String sTemp = "";
				for (CELOE.PartialDefinition pdef : counterPartialDefinitions) {
					count++;
					sTemp += ("(" + pdef.getDescription().getLength() + ", " + df.format(pdef.getCoverage()) + "); ");
					if ((count % 10) == 0) {
						outputWriter(sTemp);
						sTemp = "";
					}
				}
				System.out.println("\n----------------------");
				
				//-----------------------------------------------------
				//3. re-assign the pos/neg and restart the learner
				//-----------------------------------------------------
				Set<String> pos = Datastructures.individualSetToStringSet(trainingSetsPos
						.get(currFold));
				Set<String> neg = Datastructures.individualSetToStringSet(trainingSetsNeg
						.get(currFold));
				
				lp.setPositiveExamples(trainingSetsPos.get(currFold));
				lp.setNegativeExamples(trainingSetsNeg.get(currFold));

				try {
					lp.init();
					la.init();
				} catch (ComponentInitException e) {
					e.printStackTrace();
				}
								
				((CELOE)la).setNoisePercentage(orgNoise);
				
				outputWriter("** Phase 2 - Learning the main concept");
				outputWriter("Noise: " + orgNoise + "%");
				
				

				//---------------------------
				//start learning
				//---------------------------
				long algorithmStartTime = System.nanoTime();
				la.start();
				long algorithmDuration = System.nanoTime() - algorithmStartTime;
				runtime.addNumber(algorithmDuration / (double) 1000000000);
				
				fortifiedRuntime.addNumber((algorithmDuration1 + algorithmDuration)/1000000000d);
				
				//----------------------------
				//finish learning
				//----------------------------
							
				Description concept = la.getCurrentlyBestDescription();
				
				
				Set<Individual> curFoldPosTestSet = testSetsPos.get(currFold);
				Set<Individual> curFoldNegTestSet = testSetsNeg.get(currFold); 

				//calculate testing coverage
				Set<Individual> tmp = rs.hasType(concept, curFoldPosTestSet);
				Set<Individual> tmp2 = Helper.difference(curFoldPosTestSet, tmp);
				Set<Individual> tmp3 = rs.hasType(concept, curFoldNegTestSet);

				outputWriter("test set errors pos (" + tmp2.size() + "): " + tmp2);
				outputWriter("test set errors neg (" + tmp3.size() + "): " + tmp3);

				
				// calculate training accuracies
				int trainingCorrectPosClassified = getCorrectPosClassified(rs, concept,
						trainingSetsPos.get(currFold));
				int trainingCorrectNegClassified = getCorrectNegClassified(rs, concept,
						trainingSetsNeg.get(currFold));
				int trainingCorrectExamples = trainingCorrectPosClassified
						+ trainingCorrectNegClassified;
				double trainingAccuracy = 100 * ((double) trainingCorrectExamples / (trainingSetsPos
						.get(currFold).size() + trainingSetsNeg.get(currFold).size()));
				
				accuracyTraining.addNumber(trainingAccuracy);
				
				
				double trainingCompleteness = 100*(double)trainingCorrectPosClassified/trainingSetsPos.get(currFold).size();
				double trainingCorrectness = 100*(double)trainingCorrectNegClassified/trainingSetsNeg.get(currFold).size();
				
				trainingCompletenessStat.addNumber(trainingCompleteness);
				trainingCorrectnessStat.addNumber(trainingCorrectness);

				
				// calculate test accuracies
				int correctPosClassified = curFoldPosTestSet.size() - tmp2.size();	//getCorrectPosClassified(rs, concept,	curFoldPosTestSet);
				int correctNegClassified = curFoldNegTestSet.size() - tmp3.size();	//getCorrectNegClassified(rs, concept,	curFoldNegTestSet);
				int correctExamples = correctPosClassified + correctNegClassified;
				double currAccuracy = 100 * ((double) correctExamples / (curFoldPosTestSet.size() + 
						curFoldNegTestSet.size()));
				accuracy.addNumber(currAccuracy);
				
				double testingCompleteness = 100*(double)correctPosClassified/curFoldPosTestSet.size();
				double testingCorrectness = 100*(double)correctNegClassified/curFoldNegTestSet.size();
				
				testingCompletenessStat.addNumber(testingCompleteness);
				testingCorrectnessStat.addNumber(testingCorrectness);
				
				// calculate training F-Score
				int negAsPosTraining = rs.hasType(concept, trainingSetsNeg.get(currFold)).size();
				double precisionTraining = trainingCorrectPosClassified + negAsPosTraining == 0 ? 0
						: trainingCorrectPosClassified
								/ (double) (trainingCorrectPosClassified + negAsPosTraining);
				double recallTraining = trainingCorrectPosClassified
						/ (double) trainingSetsPos.get(currFold).size();
				double fMeasureTrainingFold = 100 * Heuristics.getFScore(recallTraining,
						precisionTraining); 
				fMeasureTraining.addNumber(fMeasureTrainingFold);
				
				//---------------------------				
				// calculate test F-Score
				//---------------------------
				int negAsPos = rs.hasType(concept, curFoldNegTestSet).size();
				double precision = correctPosClassified + negAsPos == 0 ? 0 : correctPosClassified
						/ (double) (correctPosClassified + negAsPos);
				double recall = correctPosClassified / (double) curFoldPosTestSet.size();
				// System.out.println(precision);System.out.println(recall);
				double fMeasureTestingFold = 100 * Heuristics.getFScore(recall, precision); 
				fMeasure.addNumber(fMeasureTestingFold);
				
				length.addNumber(concept.getLength());
				
				totalNumberOfDescriptions.addNumber(la.getTotalNumberOfDescriptionsGenerated());
				
				//------------------------------
				// fortification				
				//------------------------------
				//if there exists covered negative examples ==> check if there are any counter partial definitions 
				//can be used to remove covered negative examples
				
				int fixedNeg = 0;
				int fixedPos = 0;
				int selectedCpdef = 0;
				int totalSelectedCpdefLength = 0;
				double avgTrainingCoverage = 0;
				
				TreeSet<CELOE.PartialDefinition> selectedCounterPartialDefinitions = new TreeSet<CELOE.PartialDefinition>(new CoverageComparator());
				
				if (tmp3.size() > 0) {

					//tmp3: set of covered negative examples
					//tmp2: set of uncovered positive examples
					
					TreeSet<Individual> tempCoveredNeg = new TreeSet<Individual>(new URIComparator());
					tempCoveredNeg.addAll(tmp3);
					
					TreeSet<Individual> tempUncoveredPos = new TreeSet<Individual>(new URIComparator());
					tempUncoveredPos.addAll(tmp2);
					
					//check each counter partial definitions
					for (CELOE.PartialDefinition cpdef : counterPartialDefinitions) {
						
						//set of neg examples covered by the counter partial definition
						Set<Individual> desCoveredNeg = new HashSet<Individual>(rs.hasType(cpdef.getDescription(), curFoldNegTestSet));
						
						//if the current counter partial definition can help to remove some neg examples
						int oldNoOfCoveredNeg=tempCoveredNeg.size();
						if (tempCoveredNeg.removeAll(desCoveredNeg)) {
							
							cpdef.setAdditionValue(0, oldNoOfCoveredNeg - tempCoveredNeg.size());
							selectedCounterPartialDefinitions.add(cpdef);
							
							//check if it may remove some positive examples or not
							Set<Individual> desCoveredPos = new HashSet<Individual>(rs.hasType(cpdef.getDescription(), curFoldPosTestSet));
							tempUncoveredPos.addAll(desCoveredPos);
							
							//count the total number of counter partial definition selected and their total length
							selectedCpdef++;
							totalSelectedCpdefLength += cpdef.getDescription().getLength();			
							avgTrainingCoverage += cpdef.getCoverage();
						}
						
						if (tempCoveredNeg.size() == 0)
							break;
					}
					
					fixedNeg = tmp3.size() - tempCoveredNeg.size();
					fixedPos = tempUncoveredPos.size() - tmp2.size();	
					avgTrainingCoverage /= selectedCpdef;
				}
				
//				System.out.println("--------------------");
//				System.out.println("* FORTIFICATION");
//				System.out.println("\tTotal counter partial definitions: " + counterPartialDefinitions.size());
//				System.out.println("\t Total selected counter partial definitions: " + selectedCpdef);
//				System.out.println("\t Removed negative examples: " + fixedNeg + "; Removed positive examples: " + fixedPos);			
//				System.out.println("--------------------");
				
				noOfCounterPartialDefinitionUsed.addNumber(selectedCpdef);
				noOfCounterPartialDefinitions.addNumber(counterPartialDefinitions.size());
				avgCounterPartialDefinitionCoverage.addNumber(avgTrainingCoverage);
				
				
				//-----------------------------
				//fortify stat calculation
				//-----------------------------
				//def length
				double fortifiedDefinitionLength = concept.getLength() + totalSelectedCpdefLength + selectedCpdef;	//-1 from the selected cpdef and +1 for NOT
				fortifiedDefinitionLengthStat.addNumber(fortifiedDefinitionLength);
				
				double avgfortifyDefinitionLength = 0;
				
				if (selectedCpdef > 0) {
					avgfortifyDefinitionLength = (double)totalSelectedCpdefLength/selectedCpdef;				
					fortifyDefinitionLengthStat.addNumber(avgfortifyDefinitionLength);
					avgCounterPartialDefinitionLength.addNumber(totalSelectedCpdefLength/(double)selectedCpdef);
				}
				
				//accuracy
				double fortifiedAccuracy = 100 * ((double)(correctExamples + fixedNeg - fixedPos)/
						(curFoldPosTestSet.size() + curFoldNegTestSet.size()));				
				accuracyFortifyStat.addNumber(fortifiedAccuracy);
				
				//completeness
				double fortifiedCompleteness = 100 * ((double)(correctPosClassified - fixedPos)/curFoldPosTestSet.size());
				completenessFortifyStat.addNumber(fortifiedCompleteness);
				
				//correctness
				double fortifiedCorrectness = 100 * ((double)(correctNegClassified + fixedNeg)/curFoldNegTestSet.size());				
				correctnessFortifyStat.addNumber(fortifiedCorrectness);
								
				//precision, recall, f-measure
				double fortifiedPrecision = 0.0;	//percent of correct pos examples in total pos examples classified (= correct pos classified + neg as pos)
				if (((correctPosClassified - fixedPos) + (tmp3.size() - fixedNeg)) > 0)
					fortifiedPrecision = (double)(correctPosClassified - fixedPos)/
							(correctPosClassified - fixedPos + tmp3.size() - fixedNeg);	//tmp3: neg as pos <=> false pos
				
				double fortifiedRecall = (double)(correctPosClassified - fixedPos) / curFoldPosTestSet.size();
								
				double fortifiedFmeasure = 100 * Heuristics.getFScore(fortifiedRecall, fortifiedPrecision);
				fmeasureFortifyStat.addNumber(fortifiedFmeasure);
				

				//output fold stat. information
				outputWriter("Fold " + currFold + "/" + folds + ":");
				outputWriter("  training: " + (pos.size() - tmp2.size()) + "/" + pos.size() + " correct positive and " + 
						(neg.size() - tmp3.size()) + "/" + neg.size() + " correct negative examples");
				
				outputWriter("  testing: " + correctPosClassified + "/"
						+ curFoldPosTestSet.size() + " correct positives, "
						+ correctNegClassified + "/" + curFoldNegTestSet.size() + " correct negatives");

				outputWriter("  runtime: " + df.format(algorithmDuration/1000000000d)	+ "s");
				outputWriter("  runtime fortified: " + df.format((algorithmDuration1+algorithmDuration)/1000000000d) + "s");
				
				outputWriter("  def. length: " + concept.getLength());
				outputWriter("  def. length fortified: " + fortifiedDefinitionLength);
				outputWriter("  def. length fortify: " + avgfortifyDefinitionLength);

				outputWriter("  F-Measure on training set: " + df.format(fMeasureTrainingFold));				
				outputWriter("  F-Measure on test set: " + df.format(fMeasureTestingFold));
				outputWriter("  F-Measure on test set fortified: " + df.format(fortifiedFmeasure));
				
				outputWriter("  concept: " + concept);
				outputWriter("  accuracy: " + df.format(currAccuracy) + "% ("
						+ "corr:" + df.format(testingCorrectness)
						+ "%, comp:" + df.format(testingCompleteness)
						+ "%)  -- training: " + df.format(trainingAccuracy)
						+ "% (corr:" + df.format(trainingCorrectness)
						+ "%, comp:" + df.format(trainingCompleteness)
						+ "%)");
				outputWriter("  accuracy fortified: " + df.format(fortifiedAccuracy) + "% ("
						+ "corr:" + df.format(fortifiedCorrectness)
						+ "%, comp:" + df.format(fortifiedCompleteness) 
						+ ")");
														
				outputWriter("  total number of descriptions: " + la.getTotalNumberOfDescriptionsGenerated());
				outputWriter("  no of counter partial def used: " + selectedCpdef);
				
		
				outputWriter("----------");
				outputWriter("Aggregate data from fold 0 to fold " + currFold + "/" + folds);
				outputWriter("  runtime celoe: " + statOutput(df, runtime, "s"));
				outputWriter("  runtime fortified: " + statOutput(df, fortifiedRuntime, "s"));
				outputWriter("  no of descriptions: " + statOutput(df, totalNumberOfDescriptions, ""));
				outputWriter("  avg. def. length: " + statOutput(df, length, ""));
				outputWriter("  avg. fortified def. length : " + statOutput(df, fortifiedDefinitionLengthStat, ""));
				outputWriter("  avg. fortify def. length : " + statOutput(df, fortifyDefinitionLengthStat, ""));
				outputWriter("  F-Measure on training set: " + statOutput(df, fMeasureTraining, "%"));
				outputWriter("  F-Measure on test set: " + statOutput(df, fMeasure, "%"));
				outputWriter("  F-Measure on test set fortified: " + statOutput(df, fmeasureFortifyStat, "%"));
				outputWriter("  predictive accuracy on training set: " + statOutput(df, accuracyTraining, "%") + 
						" -- correctness: " + statOutput(df, trainingCorrectnessStat, "%") +
						"-- completeness: " + statOutput(df, trainingCompletenessStat, "%"));
				outputWriter("  predictive accuracy on test set: " + statOutput(df, accuracy, "%") +
						" -- correctness: " + statOutput(df, testingCorrectnessStat, "%") +
						"-- completeness: " + statOutput(df, testingCompletenessStat, "%"));				
				outputWriter("  fortified accuracy on test set: " + statOutput(df, accuracyFortifyStat, "%") +
						" -- fortified correctness: " + statOutput(df, correctnessFortifyStat, "%") +
						"-- fortified completeness: " + statOutput(df, completenessFortifyStat, "%"));
				outputWriter("  total no of counter partial definition: " + statOutput(df, noOfCounterPartialDefinitions, ""));
				outputWriter("  avg. no of counter partial definition used: " + statOutput(df, noOfCounterPartialDefinitionUsed, ""));

				outputWriter("----------------------");
				outputWriter("ORTHOGONALITY testing");
				outputWriter("	[description (length, training coevrage, neg examples removed), orthogonality check]");
				outputWriter("----------------------");
				outputWriter("Learned concept: " + concept);
				outputWriter("*** All counter partial definitions: ");
				
				//convert the learned concept into OWLAPI expression
				OWLClassExpression conceptOWLAPI = OWLAPIConverter.getOWLAPIDescription(concept); 
					
				int c = 1;
				//visit all counter partial definitions
				for (CELOE.PartialDefinition cpdef : counterPartialDefinitions) {
					OWLClassExpression pdefExpr = OWLAPIConverter.getOWLAPIDescription(cpdef.getDescription());
					
					//for each counter partial definition, combine it with the learned concept and check for the satisfiability
					int orthCheck = Orthogonality.orthogonalityCheck(pelletReasoner, ontology, conceptOWLAPI, pdefExpr);
					
					orthAllCheckCount[orthCheck]++;
					
					if (currFold < 5)
						outputWriter(c++ + ". " + cpdef.getDescription() + "(" 
								+ cpdef.getDescription().getLength() + ", " + df.format(cpdef.getCoverage()) 
								+ ", " + cpdef.getAdditionValue(0) + ")" 
								+ ", orthogonality check: " + orthCheck);					
				}
				
				
				outputWriter("\n*** Selected counter partial definitions: ");				
				c = 1;
				//output the selected counter partial definition information				
				if (selectedCpdef > 0) {										
					for (CELOE.PartialDefinition cpdef : selectedCounterPartialDefinitions) {
						OWLClassExpression pdefExpr = OWLAPIConverter.getOWLAPIDescription(cpdef.getDescription());
						
						int orthCheck = Orthogonality.orthogonalityCheck(pelletReasoner, ontology, conceptOWLAPI, pdefExpr);
						
						orthSelectedCheckCount[orthCheck]++;
						
						outputWriter(c++ + ". " + cpdef.getDescription() + "(" 
								+ cpdef.getDescription().getLength() + ", " + df.format(cpdef.getCoverage()) 
								+ ", " + cpdef.getAdditionValue(0) + ")" 
								+ ", orthogonality check: " + orthCheck);					}
				}
				
				outputWriter("------orthogonality check-------");
				outputWriter("   all cpdef check resutl: " 
						+ orthAllCheckCount[0] + "," + orthAllCheckCount[1] + ", "
						+ orthAllCheckCount[2] + "," + orthAllCheckCount[3] + ", "
						+ orthAllCheckCount[4]);
				
				outputWriter("   selected cpdef check resutl: " 
						+ orthSelectedCheckCount[0] + ", " + orthSelectedCheckCount[1] + ", "
						+ orthSelectedCheckCount[2] + ", " + orthSelectedCheckCount[3] + ", "
						+ orthSelectedCheckCount[4]);	
				
				
				outputWriter("----------------------");
				
				try {
					Thread.sleep(5000);
				}
				catch (InterruptedException e) {
					e.printStackTrace();
				}

			}	//k-fold cross validation

			
			//---------------------------------
			//end of k-fold cross validation
			//output result of the k-fold 
			//---------------------------------

			//final cumulative statistical data of a run
			
			outputWriter("");
			outputWriter("Finished the " + (kk+1) + "/" + noOfRuns + " of " + folds + "-folds cross-validation.");
			outputWriter("  runtime celoe: " + statOutput(df, runtime, "s"));
			outputWriter("  runtime fortified: " + statOutput(df, fortifiedRuntime, "s"));
			outputWriter("  no of descriptions: " + statOutput(df, totalNumberOfDescriptions, ""));
			outputWriter("  avg. def. length: " + statOutput(df, length, ""));
			outputWriter("  avg. fortified def. length : " + statOutput(df, fortifiedDefinitionLengthStat, ""));
			outputWriter("  avg. fortify def. length : " + statOutput(df, fortifyDefinitionLengthStat, ""));
			outputWriter("  F-Measure on training set: " + statOutput(df, fMeasureTraining, "%"));
			outputWriter("  F-Measure on test set: " + statOutput(df, fMeasure, "%"));
			outputWriter("  F-Measure on test set fortified: " + statOutput(df, fmeasureFortifyStat, "%"));
			outputWriter("  predictive accuracy on training set: " + statOutput(df, accuracyTraining, "%") + 
					" -- correctness: " + statOutput(df, trainingCorrectnessStat, "%") +
					"-- completeness: " + statOutput(df, trainingCompletenessStat, "%"));
			outputWriter("  predictive accuracy on test set: " + statOutput(df, accuracy, "%") +
					" -- correctness: " + statOutput(df, testingCorrectnessStat, "%") +
					"-- completeness: " + statOutput(df, testingCompletenessStat, "%"));				
			outputWriter("  fortified accuracy on test set: " + statOutput(df, accuracyFortifyStat, "%") +
					" -- fortified correctness: " + statOutput(df, correctnessFortifyStat, "%") +
					"-- fortified completeness: " + statOutput(df, completenessFortifyStat, "%"));
			outputWriter("  avg. no of counter partial definition used: " + statOutput(df, noOfCounterPartialDefinitionUsed, ""));

			
			
			//this is for copying to word document
			//f-measure, accuracy, correctness, completeness, avg pdef length, no of pdef, time, no of des, no of cpdef
			outputWriter("***without fortify (f-measure, accuracy, correctness, completeness, def. length)***\n"
					+ df.format(fMeasure.getMean()) + "\n" + df.format(fMeasure.getStandardDeviation()) + "\n"
					+ df.format(accuracy.getMean()) + "\n" + df.format(accuracy.getStandardDeviation()) + "\n"
					+ df.format(testingCorrectnessStat.getMean()) + "\n" + df.format(testingCorrectnessStat.getStandardDeviation()) + "\n"
					+ df.format(testingCompletenessStat.getMean()) + "\n" + df.format(testingCompletenessStat.getStandardDeviation()) + "\n"
					+ df.format(length.getMean()) + "\n" + df.format(length.getStandardDeviation()) + "\n");
			
			
			outputWriter("***with fortify (f-measure, accuracy, correctness, completeness, fortified def. length)***\n"
					+ df.format(fmeasureFortifyStat.getMean()) + "\n" + df.format(fmeasureFortifyStat.getStandardDeviation()) + "\n"
					+ df.format(accuracyFortifyStat.getMean()) + "\n" + df.format(accuracyFortifyStat.getStandardDeviation()) + "\n"
					+ df.format(correctnessFortifyStat.getMean()) + "\n" + df.format(correctnessFortifyStat.getStandardDeviation()) + "\n"
					+ df.format(completenessFortifyStat.getMean()) + "\n" + df.format(completenessFortifyStat.getStandardDeviation()) + "\n"
					+ df.format(fortifiedDefinitionLengthStat.getMean()) + "\n" + df.format(fortifiedDefinitionLengthStat.getStandardDeviation()) + "\n");
			
			outputWriter("***Common dimensionss (no of cpdef used, runtime celoe, fortified runtime, no of des., no of cpdef.)***\n"
					+ df.format(noOfCounterPartialDefinitionUsed.getMean()) + "\n" + df.format(noOfCounterPartialDefinitionUsed.getStandardDeviation()) + "\n"
					+ df.format(runtime.getMean()) + "\n" + df.format(runtime.getStandardDeviation()) + "\n"
					+ df.format(fortifiedRuntime.getMean()) + "\n" + df.format(fortifiedRuntime.getStandardDeviation()) + "\n"
					+ df.format(totalNumberOfDescriptions.getMean()) + "\n" + df.format(totalNumberOfDescriptions.getStandardDeviation()) + "\n"
					+ df.format(noOfCounterPartialDefinitions.getMean()) + "\n" + df.format(noOfCounterPartialDefinitions.getStandardDeviation()) + "\n");

			
			
			
			
			if (noOfRuns > 1) {	
				// runtime
				runtimeAvg.addNumber(runtime.getMean());
				runtimeMax.addNumber(runtime.getMax());
				runtimeMin.addNumber(runtime.getMin());
				runtimeDev.addNumber(runtime.getStandardDeviation());
	
				defLenAvg.addNumber(length.getMean());
				defLenMax.addNumber(length.getMax());
				defLenMin.addNumber(length.getMin());
				defLenDev.addNumber(length.getStandardDeviation());
	
				trainingAccAvg.addNumber(accuracyTraining.getMean());
				trainingAccDev.addNumber(accuracyTraining.getStandardDeviation());
				trainingAccMax.addNumber(accuracyTraining.getMax());
				trainingAccMin.addNumber(accuracyTraining.getMin());
	
				trainingCorAvg.addNumber(trainingCorrectnessStat.getMean());
				trainingCorDev.addNumber(trainingCorrectnessStat.getStandardDeviation());
				trainingCorMax.addNumber(trainingCorrectnessStat.getMax());
				trainingCorMin.addNumber(trainingCorrectnessStat.getMin());
	
				trainingComAvg.addNumber(trainingCompletenessStat.getMean());
				trainingComDev.addNumber(trainingCompletenessStat.getStandardDeviation());
				trainingComMax.addNumber(trainingCompletenessStat.getMax());
				trainingComMin.addNumber(trainingCompletenessStat.getMin());
	
				testingAccAvg.addNumber(accuracy.getMean());
				testingAccMax.addNumber(accuracy.getMax());
				testingAccMin.addNumber(accuracy.getMin());
				testingAccDev.addNumber(accuracy.getStandardDeviation());
	
				testingCorAvg.addNumber(testingCorrectnessStat.getMean());
				testingCorDev.addNumber(testingCorrectnessStat.getStandardDeviation());
				testingCorMax.addNumber(testingCorrectnessStat.getMax());
				testingCorMin.addNumber(testingCorrectnessStat.getMin());
	
				testingComAvg.addNumber(testingCompletenessStat.getMean());
				testingComDev.addNumber(testingCompletenessStat.getStandardDeviation());
				testingComMax.addNumber(testingCompletenessStat.getMax());
				testingComMin.addNumber(testingCompletenessStat.getMin());
				
				testingFMesureAvg.addNumber(fMeasure.getMean());
				testingFMesureDev.addNumber(fMeasure.getStandardDeviation());
				testingFMesureMax.addNumber(fMeasure.getMax());
				testingFMesureMin.addNumber(fMeasure.getMin());
							
				trainingFMesureAvg.addNumber(fMeasureTraining.getMean());
				trainingFMesureDev.addNumber(fMeasureTraining.getStandardDeviation());
				trainingFMesureMax.addNumber(fMeasureTraining.getMax());
				trainingFMesureMin.addNumber(fMeasureTraining.getMin());
				
				noOfDescriptionsAgv.addNumber(totalNumberOfDescriptions.getMean());
				noOfDescriptionsMax.addNumber(totalNumberOfDescriptions.getMax());
				noOfDescriptionsMin.addNumber(totalNumberOfDescriptions.getMin());
				noOfDescriptionsDev.addNumber(totalNumberOfDescriptions.getStandardDeviation());
			}
		} // for kk folds

		
		if (noOfRuns > 1) {	
			outputWriter("");
			outputWriter("Finished " + noOfRuns + " time(s) of the " + folds + "-folds cross-validations");
	
			outputWriter("runtime: " + 				
					"\n\t avg.: " + statOutput(df, runtimeAvg, "s") +
					"\n\t dev.: " + statOutput(df, runtimeDev, "s") +
					"\n\t max.: " + statOutput(df, runtimeMax, "s") + 
					"\n\t min.: " + statOutput(df, runtimeMin, "s"));
			
	
			outputWriter("no of descriptions: " + 
					"\n\t avg.: " + statOutput(df, noOfDescriptionsAgv, "") +
					"\n\t dev.: " + statOutput(df, noOfDescriptionsDev, "") +
					"\n\t max.: " + statOutput(df, noOfDescriptionsMax, "") +
					"\n\t min.: " + statOutput(df, noOfDescriptionsMin, ""));
	
			outputWriter("definition length: " + 
					"\n\t avg.: " + statOutput(df, defLenAvg, "") + 
					"\n\t dev.: " + statOutput(df, defLenDev, "") +
					"\n\t max.: " + statOutput(df, defLenMax, "") + 
					"\n\t min.: " + statOutput(df, defLenMin, ""));
	
			outputWriter("accuracy on training set:" + 
					"\n\t avg.: " + statOutput(df, trainingAccAvg, "%") +
					"\n\t dev.: " + statOutput(df, trainingAccDev, "%") +
					"\n\t max.: " + statOutput(df, trainingAccMax, "%") + 
					"\n\t min.: " + statOutput(df, trainingAccMin, "%"));
	
			outputWriter("correctness on training set: " + 
					"\n\t avg.: " + statOutput(df, trainingCorAvg, "%") +
					"\n\t dev.: " + statOutput(df, trainingCorDev, "%") +
					"\n\t max.: " + statOutput(df, trainingCorMax, "%") + 
					"\n\t min.: " + statOutput(df, trainingCorMin, "%"));
	
			outputWriter("completeness on training set: " + 
					"\n\t avg.: " + statOutput(df, trainingComAvg, "%") +
					"\n\t dev.: " + statOutput(df, trainingComDev, "%") +
					"\n\t max.: " + statOutput(df, trainingComMax, "%") + 
					"\n\t min.: " + statOutput(df, trainingComMin, "%"));
			
			outputWriter("FMesure on training set: " + 
					"\n\t avg.: " + statOutput(df, trainingFMesureAvg, "%") +
					"\n\t dev.: " + statOutput(df, trainingFMesureDev, "%") +
					"\n\t max.: " + statOutput(df, trainingFMesureMax, "%") +
					"\n\t min.: " + statOutput(df, trainingFMesureMin, "%"));
	
			outputWriter("accuracy on testing set: " + 
					"\n\t avg.: " + statOutput(df, testingAccAvg, "%") + 
					"\n\t dev.: " + statOutput(df, testingAccDev, "%") +
					"\n\t max.: " + statOutput(df, testingAccMax, "%") + 
					"\n\t min.: " + statOutput(df, testingAccMin, "%"));
	
			outputWriter("correctness on testing set: " + 
					"\n\t avg.: " + statOutput(df, testingCorAvg, "%") +
					"\n\t dev.: " + statOutput(df, testingCorDev, "%") +
					"\n\t max.: " + statOutput(df, testingCorMax, "%") + 
					"\n\t min.: " + statOutput(df, testingCorMin, "%"));
	
			outputWriter("completeness on testing set: " + 
					"\n\t avg.: " + statOutput(df, testingComAvg, "%") + 
					"\n\t dev.: " + statOutput(df, testingComDev, "%") +
					"\n\t max.: " + statOutput(df, testingComMax, "%") + 
					"\n\t min.: " + statOutput(df, testingComMin, "%"));
			
			outputWriter("FMesure on testing set: " + 
					"\n\t avg.: " + statOutput(df, testingFMesureAvg, "%") +
					"\n\t dev.: " + statOutput(df, testingFMesureDev, "%") +
					"\n\t max.: " + statOutput(df, testingFMesureMax, "%") +
					"\n\t min.: " + statOutput(df, testingFMesureMin, "%"));
		}
	}



	class URIComparator implements Comparator<Individual> {
		public int compare(Individual o1, Individual o2) {
			return o1.getURI().compareTo(o2.getURI());
		}
		
	}
	
	class CoverageComparator implements Comparator<CELOE.PartialDefinition> {
		public int compare(CELOE.PartialDefinition p1, CELOE.PartialDefinition p2) {
			if (p1.getCoverage() > p2.getCoverage())
				return -1;
			else if (p1.getCoverage() < p2.getCoverage())
				return 1;
			else
				return new ConceptComparator().compare(p1.getDescription(), p2.getDescription());
				
		}
	}
}
