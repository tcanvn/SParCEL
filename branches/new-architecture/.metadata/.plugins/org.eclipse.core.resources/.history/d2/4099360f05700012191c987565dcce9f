/**
 * Copyright (C) 2007-2008, Jens Lehmann
 *
 * This file is part of DL-Learner.
 * 
 * DL-Learner is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 3 of the License, or
 * (at your option) any later version.
 *
 * DL-Learner is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *
 */
package org.dllearner.cli.ParCEL;

import java.io.File;
import java.text.DecimalFormat;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.Set;
import java.util.TreeMap;
import java.util.TreeSet;

import org.apache.log4j.Logger;
import org.dllearner.algorithms.celoe.CELOE;
import org.dllearner.cli.CrossValidation;
import org.dllearner.cli.ParCEL.ParCELExFortifiedCrossValidation.URIComparator;
import org.dllearner.core.ComponentInitException;
import org.dllearner.core.AbstractCELA;
import org.dllearner.core.AbstractReasonerComponent;
import org.dllearner.core.owl.Description;
import org.dllearner.core.owl.Individual;
import org.dllearner.core.owl.Intersection;
import org.dllearner.kb.OWLFile;
import org.dllearner.learningproblems.Heuristics;
import org.dllearner.learningproblems.PosNegLP;
import org.dllearner.utilities.Helper;
import org.dllearner.utilities.datastructures.Datastructures;
import org.dllearner.utilities.owl.ConceptComparator;
import org.dllearner.utilities.owl.OWLAPIConverter;
import org.dllearner.utilities.statistics.Stat;
import org.dllearner.utilities.Files;
import org.semanticweb.owlapi.apibinding.OWLManager;
import org.semanticweb.owlapi.model.OWLClassExpression;
import org.semanticweb.owlapi.model.OWLOntology;
import org.semanticweb.owlapi.model.OWLOntologyManager;

import com.clarkparsia.pellet.owlapiv3.PelletReasoner;
import com.clarkparsia.pellet.owlapiv3.PelletReasonerFactory;

/**
 * Performs cross validation for the given problem. Supports k-fold cross-validation and
 * leave-one-out cross-validation.
 * 
 * @author Jens Lehmann
 * 
 */
public class CELOEFortifiedCrossValidationBlind extends CrossValidation {
	

	protected Stat noOfCounterPartialDefinitions;
	protected Stat noOfCounterPartialDefinitionUsed;
	protected Stat avgCounterPartialDefinitionLength;
	protected Stat avgCounterPartialDefinitionCoverage;
	
	//fortify variables
	protected Stat fortifiedDefinitionLengthStat;
	protected Stat fortifyDefinitionLengthStat;
	
	protected Stat accuracyFortifyStat;
	protected Stat correctnessFortifyStat;
	protected Stat completenessFortifyStat;
	protected Stat fmeasureFortifyStat;
	
	protected Stat avgFortifyCoverageTraingStat;
	protected Stat avgFortifyCoverageTestStat;
	
	protected Stat fortifiedRuntime;
	
	//blind fortification
	protected Stat accuracyBlindFortifyStat;
	protected Stat correctnessBlindFortifyStat;
	protected Stat completenessBlindFortifyStat;
	protected Stat fmeasureBlindFortifyStat;
	
	protected Stat totalCPDefLengthStat;
	protected Stat avgCPDefLengthStat;
	
	//---------
	Logger logger = Logger.getLogger(this.getClass());

	public CELOEFortifiedCrossValidationBlind() {

	}
	
	public CELOEFortifiedCrossValidationBlind(AbstractCELA la, PosNegLP lp, AbstractReasonerComponent rs, int folds,
			boolean leaveOneOut) {

		this(la, lp, rs, folds, leaveOneOut, 1);

	}

	public CELOEFortifiedCrossValidationBlind(AbstractCELA la, PosNegLP lp, AbstractReasonerComponent rs, int folds,
			boolean leaveOneOut, int noOfRuns) {

		DecimalFormat df = new DecimalFormat();

		// the training and test sets used later on
		List<Set<Individual>> trainingSetsPos = new LinkedList<Set<Individual>>();
		List<Set<Individual>> trainingSetsNeg = new LinkedList<Set<Individual>>();
		List<Set<Individual>> testSetsPos = new LinkedList<Set<Individual>>();
		List<Set<Individual>> testSetsNeg = new LinkedList<Set<Individual>>();

		// get examples and shuffle them too
		Set<Individual> posExamples = ((PosNegLP) lp).getPositiveExamples();
		List<Individual> posExamplesList = new LinkedList<Individual>(posExamples);
		Collections.shuffle(posExamplesList, new Random(1));
		Set<Individual> negExamples = ((PosNegLP) lp).getNegativeExamples();
		List<Individual> negExamplesList = new LinkedList<Individual>(negExamples);
		Collections.shuffle(negExamplesList, new Random(2));

		// sanity check whether nr. of folds makes sense for this benchmark
		if (!leaveOneOut && (posExamples.size() < folds && negExamples.size() < folds)) {
			System.out.println("The number of folds is higher than the number of "
					+ "positive/negative examples. This can result in empty test sets. Exiting.");
			System.exit(0);
		}

		// calculating where to split the sets, ; note that we split
		// positive and negative examples separately such that the
		// distribution of positive and negative examples remains similar
		// (note that there are better but more complex ways to implement this,
		// which guarantee that the sum of the elements of a fold for pos
		// and neg differs by at most 1 - it can differ by 2 in our implementation,
		// e.g. with 3 folds, 4 pos. examples, 4 neg. examples)
		int[] splitsPos = calculateSplits(posExamples.size(), folds);
		int[] splitsNeg = calculateSplits(negExamples.size(), folds);
		
		//for orthogonality check
		long orthAllCheckCount[] = new long[5];
		orthAllCheckCount[0] = orthAllCheckCount[1] = orthAllCheckCount[2] = orthAllCheckCount[3] = orthAllCheckCount[4] = 0;
		
		long orthSelectedCheckCount[] = new long[5];
		orthSelectedCheckCount[0] = orthSelectedCheckCount[1] = orthSelectedCheckCount[2] = orthSelectedCheckCount[3] = orthSelectedCheckCount[4] = 0;

		
		// calculating training and test sets
		for (int i = 0; i < folds; i++) {
			Set<Individual> testPos = getTestingSet(posExamplesList, splitsPos, i);
			Set<Individual> testNeg = getTestingSet(negExamplesList, splitsNeg, i);
			testSetsPos.add(i, testPos);
			testSetsNeg.add(i, testNeg);
			trainingSetsPos.add(i, getTrainingSet(posExamples, testPos));
			trainingSetsNeg.add(i, getTrainingSet(negExamples, testNeg));
		}

		// ---------------------------------
		// k-fold cross validation
		// ---------------------------------

		Stat runtimeAvg = new Stat();
		Stat runtimeMax = new Stat();
		Stat runtimeMin = new Stat();
		Stat runtimeDev = new Stat();

		Stat defLenAvg = new Stat();
		Stat defLenDev = new Stat();
		Stat defLenMax = new Stat();
		Stat defLenMin = new Stat();

		Stat trainingAccAvg = new Stat();
		Stat trainingAccDev = new Stat();
		Stat trainingAccMax = new Stat();
		Stat trainingAccMin = new Stat();

		Stat trainingCorAvg = new Stat();
		Stat trainingCorDev = new Stat();
		Stat trainingCorMax = new Stat();
		Stat trainingCorMin = new Stat();

		Stat trainingComAvg = new Stat();
		Stat trainingComDev = new Stat();
		Stat trainingComMax = new Stat();
		Stat trainingComMin = new Stat();

		Stat testingAccAvg = new Stat();
		Stat testingAccMax = new Stat();
		Stat testingAccMin = new Stat();
		Stat testingAccDev = new Stat();

		Stat testingCorAvg = new Stat();
		Stat testingCorDev = new Stat();
		Stat testingCorMax = new Stat();
		Stat testingCorMin = new Stat();

		Stat testingComAvg = new Stat();
		Stat testingComDev = new Stat();
		Stat testingComMax = new Stat();
		Stat testingComMin = new Stat();
		
		Stat testingFMesureAvg = new Stat();
		Stat testingFMesureDev = new Stat();
		Stat testingFMesureMax = new Stat();
		Stat testingFMesureMin = new Stat();
		
		Stat trainingFMesureAvg = new Stat();
		Stat trainingFMesureDev = new Stat();
		Stat trainingFMesureMax = new Stat();
		Stat trainingFMesureMin = new Stat();
		
		Stat noOfDescriptionsAgv = new Stat();
		Stat noOfDescriptionsMax = new Stat();
		Stat noOfDescriptionsMin = new Stat();
		Stat noOfDescriptionsDev = new Stat();
		
		//fortify stat. variables
		Stat noOfCounterPartialDefinitionsAvg = new Stat();
		Stat noOfCounterPartialDefinitionsDev = new Stat();
		Stat noOfCounterPartialDefinitionsMax = new Stat();
		Stat noOfCounterPartialDefinitionsMin = new Stat();
		
		Stat noOfCounterPartialDefinitionsUsedAvg = new Stat();
		Stat noOfCounterPartialDefinitionsUsedDev = new Stat();
		Stat noOfCounterPartialDefinitionsUsedMax = new Stat();
		Stat noOfCounterPartialDefinitionsUsedMin = new Stat();
		
		Stat avgCounterPartialDefinitionLengthAvg = new Stat();
		Stat avgCounterPartialDefinitionLengthDev = new Stat();
		Stat avgCounterPartialDefinitionLengthMax = new Stat();
		Stat avgCounterPartialDefinitionLengthMin = new Stat();
		
		
		Stat avgFirtifiedDefinitionLengthAvg = new Stat();
		Stat avgFirtifiedDefinitionLengthDev = new Stat();
		Stat avgFirtifiedDefinitionLengthMax = new Stat();
		Stat avgFirtifiedDefinitionLengthMin = new Stat();
		
		Stat accuracyFortifyAvg = new Stat();
		Stat accuracyFortifyDev = new Stat();
		Stat accuracyFortifyMax = new Stat();
		Stat accuracyFortifyMin = new Stat();
		
		Stat correctnessFortifyAvg = new Stat();
		Stat correctnessFortifyDev = new Stat();
		Stat correctnessFortifyMax = new Stat();
		Stat correctnessFortifyMin = new Stat();
		
		Stat completenessFortifyAvg = new Stat();
		Stat completenessFortifyDev = new Stat();
		Stat completenessFortifyMax = new Stat();
		Stat completenessFortifyMin = new Stat();		
		
		Stat fmeasureFortifyAvg = new Stat();
		Stat fmeasureFortifyDev = new Stat();
		Stat fmeasureFortifyMax = new Stat();
		Stat fmeasureFortifyMin = new Stat();
		
		Stat avgFortifyCoverageTraingAvg = new Stat();
		Stat avgFortifyCoverageTraingDev = new Stat();
		Stat avgFortifyCoverageTraingMax = new Stat();
		Stat avgFortifyCoverageTraingMin = new Stat();
		
		Stat avgFortifyCoverageTestAvg = new Stat();	
		Stat avgFortifyCoverageTestDev = new Stat();
		Stat avgFortifyCoverageTestMax = new Stat();
		Stat avgFortifyCoverageTestMin = new Stat();
		

		long ontologyLoadStarttime = System.nanoTime();		
		OWLOntologyManager manager = OWLManager.createOWLOntologyManager();
		OWLOntology ontology = ((OWLFile)la.getReasoner().getSources().iterator().next()).createOWLOntology(manager);			
		outputWriter("Ontology created, axiom count: " + ontology.getAxiomCount());
		PelletReasoner pelletReasoner = PelletReasonerFactory.getInstance().createReasoner(ontology);
		outputWriter("Pellet creared and binded with the ontology: " + pelletReasoner.getReasonerName());
		long ontologyLoadDuration = System.nanoTime() - ontologyLoadStarttime;
		outputWriter("Total time for creating and binding ontology: " + ontologyLoadDuration/1000000000d + "ms");

		
		for (int kk = 0; kk < noOfRuns; kk++) {

			//stat. variables for each fold ==> need to be re-created after each fold
			runtime = new Stat();
			length = new Stat();
			accuracyTraining = new Stat();
			trainingCorrectnessStat = new Stat();
			trainingCompletenessStat = new Stat();
			accuracy = new Stat();
			testingCorrectnessStat = new Stat();
			testingCompletenessStat = new Stat();
			fMeasure = new Stat();
			fMeasureTraining = new Stat();
			
			noOfCounterPartialDefinitions = new Stat();
			noOfCounterPartialDefinitionUsed = new Stat();
			avgCounterPartialDefinitionLength = new Stat();
			avgCounterPartialDefinitionCoverage = new Stat();
			
			totalNumberOfDescriptions = new Stat();
			
			//fortify variables
			fortifiedDefinitionLengthStat = new Stat();
			fortifyDefinitionLengthStat = new Stat();
			
			accuracyFortifyStat = new Stat();
			correctnessFortifyStat = new Stat();
			completenessFortifyStat = new Stat();
			fmeasureFortifyStat = new Stat();
			
			avgFortifyCoverageTraingStat = new Stat();
			avgFortifyCoverageTestStat = new Stat();	
			
			fortifiedRuntime = new Stat();
			
			//blind fortification
			accuracyBlindFortifyStat = new Stat();
			correctnessBlindFortifyStat = new Stat();
			completenessBlindFortifyStat = new Stat();
			fmeasureBlindFortifyStat = new Stat();
			
			totalCPDefLengthStat = new Stat();
			avgCPDefLengthStat = new Stat();

			// run the algorithm
			for (int currFold = 0; currFold < folds; currFold++) {
				
				outputWriter("//---------------\n" + "// Fold " + currFold + "/" + folds + "\n//---------------");
				
				//1. reserve the pos/neg examples and start the learner to get the counter partial definitions
				//2. store the counter partial definitions
				//3. reserve the pos/neg back to the original set and start the learner again to get the definition
				//4. do the test step and apply the fortification if necessary 

				
				//-----------------------------------------------------
				//1. reverse the pos/neg and let the learner starts  
				//----------------------------------------------------- 
				lp.setNegativeExamples(trainingSetsPos.get(currFold));
				lp.setPositiveExamples(trainingSetsNeg.get(currFold));

				try {
					lp.init();
					la.init();
				} catch (ComponentInitException e) {
					e.printStackTrace();
				}
				
				double orgNoise = ((CELOE)la).getNoisePercentage();
				
				outputWriter("** Phase 1 - Learning counter partial definition");
				outputWriter("Noise: 98%");
				
				((CELOE)la).setNoisePercentage(98);

				long algorithmStartTime1 = System.nanoTime();
				la.start();
				long algorithmDuration1 = System.nanoTime() - algorithmStartTime1;
								
				//2. store the counter partial definitions
				TreeSet<CELOE.PartialDefinition> counterPartialDefinitions = new TreeSet<CELOE.PartialDefinition>(new CoverageComparator()); 
				
				counterPartialDefinitions.addAll(((CELOE)la).getPartialDefinitions());			
				
				outputWriter("Finish learning, number of partial definitions: " + counterPartialDefinitions.size());
				outputWriter("CPDEF length and coverage (length, coverage): ");
				int count=1;
				String sTemp = "";
				for (CELOE.PartialDefinition pdef : counterPartialDefinitions) {
					count++;
					sTemp += ("(" + pdef.getDescription().getLength() + ", " + df.format(pdef.getCoverage()) + "); ");
					if ((count % 10) == 0) {
						outputWriter(sTemp);
						sTemp = "";
					}
				}
				System.out.println("\n----------------------");
				
				//-----------------------------------------------------
				//3. re-assign the pos/neg and restart the learner
				//-----------------------------------------------------
				//Set<String> pos = Datastructures.individualSetToStringSet(trainingSetsPos
				//		.get(currFold));
				//Set<String> neg = Datastructures.individualSetToStringSet(trainingSetsNeg
				//		.get(currFold));
				
				lp.setPositiveExamples(trainingSetsPos.get(currFold));
				lp.setNegativeExamples(trainingSetsNeg.get(currFold));

				try {
					lp.init();
					la.init();
				} catch (ComponentInitException e) {
					e.printStackTrace();
				}
								
				((CELOE)la).setNoisePercentage(orgNoise);
				
				outputWriter("** Phase 2 - Learning the main concept");
				outputWriter("Noise: " + orgNoise + "%");
				
				

				//---------------------------
				//start learning
				//---------------------------
				long algorithmStartTime = System.nanoTime();
				la.start();
				long algorithmDuration = System.nanoTime() - algorithmStartTime;
				runtime.addNumber(algorithmDuration / (double) 1000000000);
				
				fortifiedRuntime.addNumber((algorithmDuration1 + algorithmDuration)/1000000000d);
				
				//----------------------------
				//finish learning
				//----------------------------
							
				Description concept = la.getCurrentlyBestDescription();
				
				
				// calculate training accuracies, f-measure, etc.
				Set<Individual> curFoldPosTrainingSet = testSetsPos.get(currFold);
				Set<Individual> curFoldNegTrainingSet = testSetsNeg.get(currFold); 

				
				int trainingCorrectPosClassified = getCorrectPosClassified(rs, concept,	curFoldPosTrainingSet);
				int trainingCorrectNegClassified = getCorrectNegClassified(rs, concept,	curFoldNegTrainingSet);
				int trainingCorrectExamples = trainingCorrectPosClassified
						+ trainingCorrectNegClassified;
				double trainingAccuracy = 100 * ((double) trainingCorrectExamples / 
						(curFoldPosTrainingSet.size() + curFoldNegTrainingSet.size()));
				
				accuracyTraining.addNumber(trainingAccuracy);
				
				
				double trainingCompleteness = 100*(double)trainingCorrectPosClassified/trainingSetsPos.get(currFold).size();
				double trainingCorrectness = 100*(double)trainingCorrectNegClassified/trainingSetsNeg.get(currFold).size();
				
				trainingCompletenessStat.addNumber(trainingCompleteness);
				trainingCorrectnessStat.addNumber(trainingCorrectness);

				// calculate training F-Score
				int negAsPosTraining = rs.hasType(concept, curFoldNegTrainingSet).size();
				double precisionTraining = trainingCorrectPosClassified + negAsPosTraining == 0 ? 0
						: trainingCorrectPosClassified	/ (double) (trainingCorrectPosClassified + negAsPosTraining);
				double recallTraining = trainingCorrectPosClassified / (double) curFoldPosTrainingSet.size();
				double fMeasureTrainingFold = 100 * Heuristics.getFScore(recallTraining, precisionTraining);
				
				fMeasureTraining.addNumber(fMeasureTrainingFold);
				
				
				//---------------------
				//test set
				//---------------------
				Set<Individual> curFoldPosTestSet = testSetsPos.get(currFold);
				Set<Individual> curFoldNegTestSet = testSetsNeg.get(currFold); 
				
				Set<Individual> positiveCoveredByCpdef = new HashSet<Individual>();
				Set<Individual> negativeCoveredByCpdef = new HashSet<Individual>();

				//calculate testing coverage
				Set<Individual> cpTest = rs.hasType(concept, curFoldPosTestSet);			//cp
				Set<Individual> upTest = Helper.difference(curFoldPosTestSet, cpTest);		//up
				Set<Individual> cnTest = rs.hasType(concept, curFoldNegTestSet);			//cn

				outputWriter("test set errors pos (" + upTest.size() + "): " + upTest);
				outputWriter("test set errors neg (" + cnTest.size() + "): " + cnTest);

				//blind fortification
				for (CELOE.PartialDefinition cpdef : counterPartialDefinitions) {
					positiveCoveredByCpdef.addAll(rs.hasType(cpdef.getDescription(), curFoldPosTestSet));
					negativeCoveredByCpdef.addAll(rs.hasType(cpdef.getDescription(), curFoldNegTestSet));
				}
				
				// calculate test accuracies
				int correctPosClassified = cpTest.size(); 	//covered positive examples		//curFoldPosTestSet.size() - upTest.size();	//getCorrectPosClassified(rs, concept,	curFoldPosTestSet);
				int correctNegClassified = curFoldNegTestSet.size() - cnTest.size();		//getCorrectNegClassified(rs, concept,	curFoldNegTestSet);
				
				int correctExamples = correctPosClassified + correctNegClassified;
				double currAccuracy = 100 * ((double) correctExamples / (curFoldPosTestSet.size() + 
						curFoldNegTestSet.size()));
				
				accuracy.addNumber(currAccuracy);
				
				double testingCompleteness = 100*(double)correctPosClassified/curFoldPosTestSet.size();
				double testingCorrectness = 100*(double)correctNegClassified/curFoldNegTestSet.size();
				
				testingCompletenessStat.addNumber(testingCompleteness);
				testingCorrectnessStat.addNumber(testingCorrectness);
				

				//---------------------------				
				// calculate test F-Score
				//---------------------------
				int negAsPos = cnTest.size(); 	//rs.hasType(concept, curFoldNegTestSet).size();	//cn
				double precision = correctPosClassified + negAsPos == 0 ? 0 : correctPosClassified
						/ (double) (correctPosClassified + negAsPos);
				double recall = correctPosClassified / (double) curFoldPosTestSet.size();
				
				double fMeasureTestingFold = 100 * Heuristics.getFScore(recall, precision); 
				fMeasure.addNumber(fMeasureTestingFold);
				
				length.addNumber(concept.getLength());
				
				totalNumberOfDescriptions.addNumber(la.getTotalNumberOfDescriptionsGenerated());
				
				
				//------------------------------
				// fortification				
				//------------------------------
				//if there exists covered negative examples ==> check if there are any counter partial definitions 
				//can be used to remove covered negative examples
				
				int fixedNeg = 0;
				int fixedPos = 0;
				int selectedCpdef = 0;
				int totalSelectedCpdefLength = 0;
				double avgTrainingCoverage = 0;
				
				TreeSet<CELOE.PartialDefinition> selectedCounterPartialDefinitions = new TreeSet<CELOE.PartialDefinition>(new CoverageComparator());
				
				if (cnTest.size() > 0) {
					
					TreeSet<Individual> tempCoveredNeg = new TreeSet<Individual>(new URIComparator());
					tempCoveredNeg.addAll(cnTest);
					
					TreeSet<Individual> tempUncoveredPos = new TreeSet<Individual>(new URIComparator());
					tempUncoveredPos.addAll(upTest);
					
					//check each counter partial definitions
					for (CELOE.PartialDefinition cpdef : counterPartialDefinitions) {
						
						//set of neg examples covered by the counter partial definition
						Set<Individual> desCoveredNeg = new HashSet<Individual>(rs.hasType(cpdef.getDescription(), curFoldNegTestSet));
						
						//if the current counter partial definition can help to remove some neg examples
						int oldNoOfCoveredNeg=tempCoveredNeg.size();
						if (tempCoveredNeg.removeAll(desCoveredNeg)) {
							
							cpdef.setAdditionValue(0, oldNoOfCoveredNeg - tempCoveredNeg.size());
							selectedCounterPartialDefinitions.add(cpdef);
							
							//check if it may remove some positive examples or not
							Set<Individual> desCoveredPos = new HashSet<Individual>(rs.hasType(cpdef.getDescription(), curFoldPosTestSet));
							tempUncoveredPos.addAll(desCoveredPos);
							
							//count the total number of counter partial definition selected and their total length
							selectedCpdef++;
							totalSelectedCpdefLength += cpdef.getDescription().getLength();			
							avgTrainingCoverage += cpdef.getCoverage();
						}
						
						if (tempCoveredNeg.size() == 0)
							break;
					}
					
					fixedNeg = cnTest.size() - tempCoveredNeg.size();
					fixedPos = tempUncoveredPos.size() - upTest.size();	
					avgTrainingCoverage /= selectedCpdef;
				}
				
				
				noOfCounterPartialDefinitionUsed.addNumber(selectedCpdef);
				noOfCounterPartialDefinitions.addNumber(counterPartialDefinitions.size());
				avgCounterPartialDefinitionCoverage.addNumber(avgTrainingCoverage);
				
				
				//-----------------------------
				//fortify stat calculation
				//-----------------------------
				//def length
				double fortifiedDefinitionLength = concept.getLength() + totalSelectedCpdefLength + selectedCpdef;	//-1 from the selected cpdef and +1 for NOT
				fortifiedDefinitionLengthStat.addNumber(fortifiedDefinitionLength);
				
				double avgfortifyDefinitionLength = 0;
				
				if (selectedCpdef > 0) {
					avgfortifyDefinitionLength = (double)totalSelectedCpdefLength/selectedCpdef;				
					fortifyDefinitionLengthStat.addNumber(avgfortifyDefinitionLength);
					avgCounterPartialDefinitionLength.addNumber(totalSelectedCpdefLength/(double)selectedCpdef);
				}
				
				//accuracy
				double fortifiedAccuracy = 100 * ((double)(correctExamples + fixedNeg - fixedPos)/
						(curFoldPosTestSet.size() + curFoldNegTestSet.size()));				
				accuracyFortifyStat.addNumber(fortifiedAccuracy);
				
				//completeness
				double fortifiedCompleteness = 100 * ((double)(correctPosClassified - fixedPos)/curFoldPosTestSet.size());
				completenessFortifyStat.addNumber(fortifiedCompleteness);
				
				//correctness
				double fortifiedCorrectness = 100 * ((double)(correctNegClassified + fixedNeg)/curFoldNegTestSet.size());				
				correctnessFortifyStat.addNumber(fortifiedCorrectness);
								
				//precision, recall, f-measure
				double fortifiedPrecision = 0.0;	//percent of correct pos examples in total pos examples classified (= correct pos classified + neg as pos)
				if (((correctPosClassified - fixedPos) + (cnTest.size() - fixedNeg)) > 0)
					fortifiedPrecision = (double)(correctPosClassified - fixedPos)/
							(correctPosClassified - fixedPos + cnTest.size() - fixedNeg);	//tmp3: neg as pos <=> false pos
				
				double fortifiedRecall = (double)(correctPosClassified - fixedPos) / curFoldPosTestSet.size();
								
				double fortifiedFmeasure = 100 * Heuristics.getFScore(fortifiedRecall, fortifiedPrecision);
				fmeasureFortifyStat.addNumber(fortifiedFmeasure);
				
				
				//---------------------------------
				// blind fortification
				//---------------------------------
				//get the set of pos and neg (in the test set) covered by counter partial definition
				Set<Individual> cpdefPositiveCovered = new HashSet<Individual>();
				Set<Individual> cpdefNegativeCovered = new HashSet<Individual>();
								
				OWLClassExpression conceptOWLAPI = OWLAPIConverter.getOWLAPIDescription(concept);
				
				long totalCPDefLength = 0;

				for (CELOE.PartialDefinition cpdef : counterPartialDefinitions) {
					Set<Individual> cp_tmp = rs.hasType(cpdef.getDescription(), curFoldPosTestSet);
					Set<Individual> cn_tmp = rs.hasType(cpdef.getDescription(), curFoldNegTestSet);
					
					cpdefPositiveCovered.addAll(cp_tmp);
					cpdefNegativeCovered.addAll(cn_tmp);
					
					totalCPDefLength += cpdef.getDescription().getLength();					
					
					if (cp_tmp.size() > 0) {
						OWLClassExpression pdefExpr = OWLAPIConverter.getOWLAPIDescription(cpdef.getDescription());						
						outputWriter( "   * blind fort.: " + cpdef.getDescription().toString() + "--- cp=" + cp_tmp + " --- cn=" + cn_tmp + " --- ortho. Check =" + Orthogonality.orthogonalityCheck(pelletReasoner, ontology, conceptOWLAPI, pdefExpr));
					}
					
				}
				
				outputWriter( " * blind fort.: cp=" + cpdefPositiveCovered + " --- cn=" + cpdefNegativeCovered);
				
				
				//cpdef length
				totalCPDefLengthStat.addNumber(totalCPDefLength);
				double avgCPDefLength = totalCPDefLength/(double)counterPartialDefinitions.size();
				avgCPDefLengthStat.addNumber(avgCPDefLength);
				
				
				int oldSizePosFort = cpdefPositiveCovered.size();
				int oldSizeNegFort = cpdefNegativeCovered.size();
				
				cpdefPositiveCovered.removeAll(cpTest);
				cpdefNegativeCovered.removeAll(cnTest);
				
				int commonPos = oldSizePosFort - cpdefPositiveCovered.size();
				int commonNeg = oldSizeNegFort - cpdefNegativeCovered.size();
				
				
				int cpFort = cpTest.size() - commonPos;	//positive examples covered by fortified definition
				int cnFort = cnTest.size() - commonNeg;	//negative examples covered by fortified definition
				
				//correctness = un/negSize
				double blindFortificationCorrectness = 100 *  (curFoldNegTestSet.size() - cnFort)/(double)(curFoldNegTestSet.size());
				
				//completeness = cp/posSize
				double blindFortificationCompleteness = 100 * (cpFort)/(double)curFoldPosTestSet.size();
				
				//accuracy = (cp + un)/(pos + neg)
				double blindFortificationAccuracy = 100 * (cpFort + (curFoldNegTestSet.size() - cnFort))/
						(double)(curFoldPosTestSet.size() + curFoldNegTestSet.size());
				
				//precision = right positive classified / total positive classified
				//          = cp / (cp + negAsPos)
				double blindPrecission = (cpFort + cnFort) == 0 ? 0 : cpFort / (double)(cpFort + cnFort);
				
				//recall = right positive classified / total positive
				double blindRecall = cpFort / (double)curFoldPosTestSet.size();
				
				double blindFmeasure = 100 * Heuristics.getFScore(blindRecall, blindPrecission);
				
				//stat value for blind fortification
				correctnessBlindFortifyStat.addNumber(blindFortificationCorrectness);
				completenessBlindFortifyStat.addNumber(blindFortificationCompleteness);
				accuracyBlindFortifyStat.addNumber(blindFortificationAccuracy);
				fmeasureBlindFortifyStat.addNumber(blindFmeasure);
				
				//--------------------------------
				//output fold stat. information
				//--------------------------------
				outputWriter("Fold " + currFold + "/" + folds + ":");
				outputWriter("  concept: " + concept);
				
				//training and test error
				outputWriter("  training: " + trainingCorrectPosClassified + "/" + curFoldPosTrainingSet.size() + 
						" correct positive and " + 
						(trainingCorrectNegClassified) + "/" + curFoldNegTrainingSet.size() + " correct negative examples");
				
				outputWriter("  testing: " + correctPosClassified + "/"
						+ curFoldPosTestSet.size() + " correct positives, "
						+ correctNegClassified + "/" + curFoldNegTestSet.size() + " correct negatives");

				//runtime
				outputWriter("  runtime: " + df.format(algorithmDuration/1000000000d)	+ "s");
				outputWriter("  runtime fortified: " + df.format((algorithmDuration1+algorithmDuration)/1000000000d) + "s");
				
				//def. length
				outputWriter("  def. length: " + concept.getLength());
				outputWriter("  def. length fortified: " + fortifiedDefinitionLength);
				outputWriter("  def. length fortify: " + avgfortifyDefinitionLength);
				outputWriter("  total cpdef length: " + totalCPDefLength);
				outputWriter("  avg cpdef. length: " + avgCPDefLength);

				//f-measure
				outputWriter("  F-Measure on training set: " + df.format(fMeasureTrainingFold));				
				outputWriter("  F-Measure on test set: " + df.format(fMeasureTestingFold));
				outputWriter("  F-Measure on test set fortification: " + df.format(fortifiedFmeasure));
				outputWriter("  F-measure on test set blind fortification: " + df.format(blindFmeasure));
				
				
				outputWriter("  accuracy: " + df.format(currAccuracy) + "% ("
						+ "corr:" + df.format(testingCorrectness)
						+ "%, comp:" + df.format(testingCompleteness)
						+ "%)  -- training: " + df.format(trainingAccuracy)
						+ "% (corr:" + df.format(trainingCorrectness)
						+ "%, comp:" + df.format(trainingCompleteness)
						+ "%)");
				
				outputWriter("  accuracy fortification: " + df.format(fortifiedAccuracy) + "% ("
						+ "corr:" + df.format(fortifiedCorrectness)
						+ "%, comp:" + df.format(fortifiedCompleteness) 
						+ ")");
				
				outputWriter("  accuracy blind fortification: " + df.format(blindFortificationAccuracy) + 
						"% ( corr:" + df.format(blindFortificationCorrectness)
						+ "%, comp:" + df.format(blindFortificationCompleteness) 
						+ ")");

				
				outputWriter("  total number of descriptions: " + la.getTotalNumberOfDescriptionsGenerated());
				outputWriter("  no of counter partial def used: " + selectedCpdef);
				
		
				outputWriter("----------");
				outputWriter("Aggregate data from fold 0 to fold " + currFold + "/" + folds);
				outputWriter("  runtime celoe: " + statOutput(df, runtime, "s"));
				outputWriter("  runtime fortified: " + statOutput(df, fortifiedRuntime, "s"));
				outputWriter("  no of descriptions: " + statOutput(df, totalNumberOfDescriptions, ""));
				outputWriter("  avg. def. length: " + statOutput(df, length, ""));
				outputWriter("  avg. fortified def. length : " + statOutput(df, fortifiedDefinitionLengthStat, ""));
				outputWriter("  avg. fortify def. length : " + statOutput(df, fortifyDefinitionLengthStat, ""));
				outputWriter("  F-Measure on training set: " + statOutput(df, fMeasureTraining, "%"));
				outputWriter("  F-Measure on test set: " + statOutput(df, fMeasure, "%"));
				outputWriter("  F-Measure on test set fortified: " + statOutput(df, fmeasureFortifyStat, "%"));
				outputWriter("  predictive accuracy on training set: " + statOutput(df, accuracyTraining, "%") + 
						" -- correctness: " + statOutput(df, trainingCorrectnessStat, "%") +
						" -- completeness: " + statOutput(df, trainingCompletenessStat, "%"));
				outputWriter("  predictive accuracy on test set: " + statOutput(df, accuracy, "%") +
						" -- correctness: " + statOutput(df, testingCorrectnessStat, "%") +
						" -- completeness: " + statOutput(df, testingCompletenessStat, "%"));				
				
				outputWriter("  fortified accuracy on test set: " + statOutput(df, accuracyFortifyStat, "%") +
						" -- fortified correctness: " + statOutput(df, correctnessFortifyStat, "%") +
						" -- fortified completeness: " + statOutput(df, completenessFortifyStat, "%"));

				outputWriter("  blind fortified accuracy on test set: " + statOutput(df, accuracyBlindFortifyStat, "%") +
						" -- fortified correctness: " + statOutput(df, correctnessBlindFortifyStat, "%") +
						" -- fortified completeness: " + statOutput(df, completenessBlindFortifyStat, "%"));

				outputWriter("  total no of counter partial definition: " + statOutput(df, noOfCounterPartialDefinitions, ""));
				outputWriter("  avg. no of counter partial definition used: " + statOutput(df, noOfCounterPartialDefinitionUsed, ""));

				
				//-----------------------------
				// orthogonality check
				//-----------------------------
				outputWriter("----------------------");
				outputWriter("ORTHOGONALITY testing");
				outputWriter("	[description (length, training coverage, neg examples removed, orthogonality check)]");
				outputWriter("----------------------");
				outputWriter("Learned concept: " + concept);
				outputWriter("*** All counter partial definitions: ");
				
				
				//convert the learned concept into OWLAPI expression
				//OWLClassExpression conceptOWLAPI = OWLAPIConverter.getOWLAPIDescription(concept); 
					
				int c = 1;
				//visit all counter partial definitions
				Map<Long, Integer> jaccardValueCount = new TreeMap<Long, Integer>();
				
				for (CELOE.PartialDefinition cpdef : counterPartialDefinitions) {
					OWLClassExpression pdefExpr = OWLAPIConverter.getOWLAPIDescription(cpdef.getDescription());
					
					//for each counter partial definition, combine it with the learned concept and check for the satisfiability
					int orthCheck = Orthogonality.orthogonalityCheck(pelletReasoner, ontology, conceptOWLAPI, pdefExpr);
					
					orthAllCheckCount[orthCheck]++;

					double jaccardDistance = Orthogonality.jaccardDistance(concept, cpdef.getDescription());
					
					//if (currFold < 5)
					outputWriter(c++ + ". " + cpdef.getDescription() + "(" 
							+ cpdef.getDescription().getLength() + ", " + df.format(cpdef.getCoverage()) 
							+ ", " + cpdef.getAdditionValue(0)  
							+ ", " + orthCheck 
							+ ", " + df.format(jaccardDistance) + ")"
							+ ", cp=" + rs.hasType(cpdef.getDescription(), curFoldPosTestSet)
							+ ", cn=" + rs.hasType(cpdef.getDescription(), curFoldNegTestSet)
							);		
					
					long tmp_key = Math.round(jaccardDistance * 1000);
					if (jaccardValueCount.containsKey(tmp_key)) {
						jaccardValueCount.put(tmp_key, jaccardValueCount.get(tmp_key)+1);
					}
					else
						jaccardValueCount.put(tmp_key, 1);
				}
				
				outputWriter("*** Jaccard values count:");
				for (Long value : jaccardValueCount.keySet()) 
					outputWriter(df.format(value/1000d) + ": " + jaccardValueCount.get(value));
				
				
				outputWriter("\n*** Selected counter partial definitions: ");				
				c = 1;
				jaccardValueCount.clear();
				//output the selected counter partial definition information				
				if (selectedCpdef > 0) {										
					for (CELOE.PartialDefinition cpdef : selectedCounterPartialDefinitions) {
						OWLClassExpression pdefExpr = OWLAPIConverter.getOWLAPIDescription(cpdef.getDescription());
						
						int orthCheck = Orthogonality.orthogonalityCheck(pelletReasoner, ontology, conceptOWLAPI, pdefExpr);
						
						orthSelectedCheckCount[orthCheck]++;
						
						double jaccardDistance = Orthogonality.jaccardDistance(concept, cpdef.getDescription());
						
						outputWriter(c++ + ". " + cpdef.getDescription() + "(" 
								+ cpdef.getDescription().getLength() + ", " + df.format(cpdef.getCoverage()) 
								+ ", " + cpdef.getAdditionValue(0) 
								+ ", " + orthCheck 
								+ ", " + df.format(jaccardDistance) + ")"
								+ ", cp=" + rs.hasType(cpdef.getDescription(), curFoldPosTestSet)
								+ ", cn=" + rs.hasType(cpdef.getDescription(), curFoldNegTestSet)
								);
						
					
						//outputWriter("\tFlatten: " + Orthogonality.flattenDescription(new Intersection(concept, cpdef.getDescription())));
						long tmp_key = Math.round(jaccardDistance * 1000);
						if (jaccardValueCount.containsKey(tmp_key)) {
							jaccardValueCount.put(tmp_key, jaccardValueCount.get(tmp_key)+1);
						}
						else
							jaccardValueCount.put(tmp_key, 1);
					}
					
					outputWriter("*** Jaccard values count:");
					for (Long value : jaccardValueCount.keySet()) 
						outputWriter(df.format(value/1000d) + ": " + jaccardValueCount.get(value));
							
				}				
				
				
				outputWriter("------orthogonality check-------");
				outputWriter("   all cpdef check resutl: " 
						+ orthAllCheckCount[0] + "," + orthAllCheckCount[1] + ", "
						+ orthAllCheckCount[2] + "," + orthAllCheckCount[3] + ", "
						+ orthAllCheckCount[4]);
				
				outputWriter("   selected cpdef check resutl: " 
						+ orthSelectedCheckCount[0] + ", " + orthSelectedCheckCount[1] + ", "
						+ orthSelectedCheckCount[2] + ", " + orthSelectedCheckCount[3] + ", "
						+ orthSelectedCheckCount[4]);	
				
				
				outputWriter("----------------------");
				
				try {
					Thread.sleep(5000);
				}
				catch (InterruptedException e) {
					e.printStackTrace();
				}

			}	//k-fold cross validation

			
			//---------------------------------
			//end of k-fold cross validation
			//output result of the k-fold 
			//---------------------------------

			//final cumulative statistical data of a run
			
			outputWriter("");
			outputWriter("Finished the " + (kk+1) + "/" + noOfRuns + " of " + folds + "-folds cross-validation.");
			outputWriter("  runtime celoe: " + statOutput(df, runtime, "s"));
			outputWriter("  runtime fortified: " + statOutput(df, fortifiedRuntime, "s"));
			outputWriter("  no of descriptions: " + statOutput(df, totalNumberOfDescriptions, ""));
			outputWriter("  avg. def. length: " + statOutput(df, length, ""));
			outputWriter("  avg. fortified def. length : " + statOutput(df, fortifiedDefinitionLengthStat, ""));
			outputWriter("  avg. fortify def. length : " + statOutput(df, fortifyDefinitionLengthStat, ""));
			outputWriter("  F-Measure on training set: " + statOutput(df, fMeasureTraining, "%"));
			outputWriter("  F-Measure on test set: " + statOutput(df, fMeasure, "%"));
			outputWriter("  F-Measure on test set fortification: " + statOutput(df, fmeasureFortifyStat, "%"));
			outputWriter("  F-Measure on test set blind fortification: " + statOutput(df, fmeasureBlindFortifyStat, "%"));
			
			outputWriter("  predictive accuracy on training set: " + statOutput(df, accuracyTraining, "%") + 
					" -- correctness: " + statOutput(df, trainingCorrectnessStat, "%") +
					"-- completeness: " + statOutput(df, trainingCompletenessStat, "%"));
			
			outputWriter("  predictive accuracy on test set: " + statOutput(df, accuracy, "%") +
					" -- correctness: " + statOutput(df, testingCorrectnessStat, "%") +
					"-- completeness: " + statOutput(df, testingCompletenessStat, "%"));				
			
			outputWriter("  fortified accuracy on test set: " + statOutput(df, accuracyFortifyStat, "%") +
					" -- fortified correctness: " + statOutput(df, correctnessFortifyStat, "%") +
					"-- fortified completeness: " + statOutput(df, completenessFortifyStat, "%"));

			outputWriter("  blind fortified accuracy on test set: " + statOutput(df, accuracyBlindFortifyStat, "%") +
					" -- fortified correctness: " + statOutput(df, correctnessBlindFortifyStat, "%") +
					"-- fortified completeness: " + statOutput(df, completenessBlindFortifyStat, "%"));
			
			outputWriter("  avg. no of counter partial definition used: " + statOutput(df, noOfCounterPartialDefinitionUsed, ""));

			
			
			//this is for copying to word document
			//f-measure, accuracy, correctness, completeness, avg pdef length, no of pdef, time, no of des, no of cpdef
			outputWriter("***without fortify (f-measure, accuracy, correctness, completeness, def. length)***\n"
					+ df.format(fMeasure.getMean()) + "\n" + df.format(fMeasure.getStandardDeviation()) + "\n"
					+ df.format(accuracy.getMean()) + "\n" + df.format(accuracy.getStandardDeviation()) + "\n"
					+ df.format(testingCorrectnessStat.getMean()) + "\n" + df.format(testingCorrectnessStat.getStandardDeviation()) + "\n"
					+ df.format(testingCompletenessStat.getMean()) + "\n" + df.format(testingCompletenessStat.getStandardDeviation()) + "\n"
					+ df.format(length.getMean()) + "\n" + df.format(length.getStandardDeviation()) + "\n");
			
			
			outputWriter("***with fortify (f-measure, accuracy, correctness, completeness, fortified def. length)***\n"
					+ df.format(fmeasureFortifyStat.getMean()) + "\n" + df.format(fmeasureFortifyStat.getStandardDeviation()) + "\n"
					+ df.format(accuracyFortifyStat.getMean()) + "\n" + df.format(accuracyFortifyStat.getStandardDeviation()) + "\n"
					+ df.format(correctnessFortifyStat.getMean()) + "\n" + df.format(correctnessFortifyStat.getStandardDeviation()) + "\n"
					+ df.format(completenessFortifyStat.getMean()) + "\n" + df.format(completenessFortifyStat.getStandardDeviation()) + "\n"
					+ df.format(fortifiedDefinitionLengthStat.getMean()) + "\n" + df.format(fortifiedDefinitionLengthStat.getStandardDeviation()) + "\n");
			
			outputWriter("***with blind fortify (f-measure, accuracy, correctness, completeness, total cpdef. length, avg cpdef. length)***\n"
					+ df.format(fmeasureBlindFortifyStat.getMean()) + "\n" + df.format(fmeasureBlindFortifyStat.getStandardDeviation()) + "\n"
					+ df.format(accuracyBlindFortifyStat.getMean()) + "\n" + df.format(accuracyBlindFortifyStat.getStandardDeviation()) + "\n"
					+ df.format(correctnessBlindFortifyStat.getMean()) + "\n" + df.format(correctnessBlindFortifyStat.getStandardDeviation()) + "\n"
					+ df.format(completenessBlindFortifyStat.getMean()) + "\n" + df.format(completenessBlindFortifyStat.getStandardDeviation()) + "\n"
					+ df.format(totalCPDefLengthStat.getMean()) + "\n" + df.format(totalCPDefLengthStat.getStandardDeviation()) + "\n"
					+ df.format(avgCPDefLengthStat.getMean()) + "\n" + df.format(avgCPDefLengthStat.getStandardDeviation()) + "\n"
					);

			
			outputWriter("***Common dimensions (no of cpdef used, runtime celoe, fortified runtime, no of des., no of cpdef.)***\n"
					+ df.format(noOfCounterPartialDefinitionUsed.getMean()) + "\n" + df.format(noOfCounterPartialDefinitionUsed.getStandardDeviation()) + "\n"
					+ df.format(runtime.getMean()) + "\n" + df.format(runtime.getStandardDeviation()) + "\n"
					+ df.format(fortifiedRuntime.getMean()) + "\n" + df.format(fortifiedRuntime.getStandardDeviation()) + "\n"
					+ df.format(totalNumberOfDescriptions.getMean()) + "\n" + df.format(totalNumberOfDescriptions.getStandardDeviation()) + "\n"
					+ df.format(noOfCounterPartialDefinitions.getMean()) + "\n" + df.format(noOfCounterPartialDefinitions.getStandardDeviation()) + "\n");

			
			
			if (noOfRuns > 1) {	
				// runtime
				runtimeAvg.addNumber(runtime.getMean());
				runtimeMax.addNumber(runtime.getMax());
				runtimeMin.addNumber(runtime.getMin());
				runtimeDev.addNumber(runtime.getStandardDeviation());
	
				defLenAvg.addNumber(length.getMean());
				defLenMax.addNumber(length.getMax());
				defLenMin.addNumber(length.getMin());
				defLenDev.addNumber(length.getStandardDeviation());
	
				trainingAccAvg.addNumber(accuracyTraining.getMean());
				trainingAccDev.addNumber(accuracyTraining.getStandardDeviation());
				trainingAccMax.addNumber(accuracyTraining.getMax());
				trainingAccMin.addNumber(accuracyTraining.getMin());
	
				trainingCorAvg.addNumber(trainingCorrectnessStat.getMean());
				trainingCorDev.addNumber(trainingCorrectnessStat.getStandardDeviation());
				trainingCorMax.addNumber(trainingCorrectnessStat.getMax());
				trainingCorMin.addNumber(trainingCorrectnessStat.getMin());
	
				trainingComAvg.addNumber(trainingCompletenessStat.getMean());
				trainingComDev.addNumber(trainingCompletenessStat.getStandardDeviation());
				trainingComMax.addNumber(trainingCompletenessStat.getMax());
				trainingComMin.addNumber(trainingCompletenessStat.getMin());
	
				testingAccAvg.addNumber(accuracy.getMean());
				testingAccMax.addNumber(accuracy.getMax());
				testingAccMin.addNumber(accuracy.getMin());
				testingAccDev.addNumber(accuracy.getStandardDeviation());
	
				testingCorAvg.addNumber(testingCorrectnessStat.getMean());
				testingCorDev.addNumber(testingCorrectnessStat.getStandardDeviation());
				testingCorMax.addNumber(testingCorrectnessStat.getMax());
				testingCorMin.addNumber(testingCorrectnessStat.getMin());
	
				testingComAvg.addNumber(testingCompletenessStat.getMean());
				testingComDev.addNumber(testingCompletenessStat.getStandardDeviation());
				testingComMax.addNumber(testingCompletenessStat.getMax());
				testingComMin.addNumber(testingCompletenessStat.getMin());
				
				testingFMesureAvg.addNumber(fMeasure.getMean());
				testingFMesureDev.addNumber(fMeasure.getStandardDeviation());
				testingFMesureMax.addNumber(fMeasure.getMax());
				testingFMesureMin.addNumber(fMeasure.getMin());
							
				trainingFMesureAvg.addNumber(fMeasureTraining.getMean());
				trainingFMesureDev.addNumber(fMeasureTraining.getStandardDeviation());
				trainingFMesureMax.addNumber(fMeasureTraining.getMax());
				trainingFMesureMin.addNumber(fMeasureTraining.getMin());
				
				noOfDescriptionsAgv.addNumber(totalNumberOfDescriptions.getMean());
				noOfDescriptionsMax.addNumber(totalNumberOfDescriptions.getMax());
				noOfDescriptionsMin.addNumber(totalNumberOfDescriptions.getMin());
				noOfDescriptionsDev.addNumber(totalNumberOfDescriptions.getStandardDeviation());
			}
		} // for kk folds

		
		if (noOfRuns > 1) {	
			outputWriter("");
			outputWriter("Finished " + noOfRuns + " time(s) of the " + folds + "-folds cross-validations");
	
			outputWriter("runtime: " + 				
					"\n\t avg.: " + statOutput(df, runtimeAvg, "s") +
					"\n\t dev.: " + statOutput(df, runtimeDev, "s") +
					"\n\t max.: " + statOutput(df, runtimeMax, "s") + 
					"\n\t min.: " + statOutput(df, runtimeMin, "s"));
			
	
			outputWriter("no of descriptions: " + 
					"\n\t avg.: " + statOutput(df, noOfDescriptionsAgv, "") +
					"\n\t dev.: " + statOutput(df, noOfDescriptionsDev, "") +
					"\n\t max.: " + statOutput(df, noOfDescriptionsMax, "") +
					"\n\t min.: " + statOutput(df, noOfDescriptionsMin, ""));
	
			outputWriter("definition length: " + 
					"\n\t avg.: " + statOutput(df, defLenAvg, "") + 
					"\n\t dev.: " + statOutput(df, defLenDev, "") +
					"\n\t max.: " + statOutput(df, defLenMax, "") + 
					"\n\t min.: " + statOutput(df, defLenMin, ""));
	
			outputWriter("accuracy on training set:" + 
					"\n\t avg.: " + statOutput(df, trainingAccAvg, "%") +
					"\n\t dev.: " + statOutput(df, trainingAccDev, "%") +
					"\n\t max.: " + statOutput(df, trainingAccMax, "%") + 
					"\n\t min.: " + statOutput(df, trainingAccMin, "%"));
	
			outputWriter("correctness on training set: " + 
					"\n\t avg.: " + statOutput(df, trainingCorAvg, "%") +
					"\n\t dev.: " + statOutput(df, trainingCorDev, "%") +
					"\n\t max.: " + statOutput(df, trainingCorMax, "%") + 
					"\n\t min.: " + statOutput(df, trainingCorMin, "%"));
	
			outputWriter("completeness on training set: " + 
					"\n\t avg.: " + statOutput(df, trainingComAvg, "%") +
					"\n\t dev.: " + statOutput(df, trainingComDev, "%") +
					"\n\t max.: " + statOutput(df, trainingComMax, "%") + 
					"\n\t min.: " + statOutput(df, trainingComMin, "%"));
			
			outputWriter("FMesure on training set: " + 
					"\n\t avg.: " + statOutput(df, trainingFMesureAvg, "%") +
					"\n\t dev.: " + statOutput(df, trainingFMesureDev, "%") +
					"\n\t max.: " + statOutput(df, trainingFMesureMax, "%") +
					"\n\t min.: " + statOutput(df, trainingFMesureMin, "%"));
	
			outputWriter("accuracy on testing set: " + 
					"\n\t avg.: " + statOutput(df, testingAccAvg, "%") + 
					"\n\t dev.: " + statOutput(df, testingAccDev, "%") +
					"\n\t max.: " + statOutput(df, testingAccMax, "%") + 
					"\n\t min.: " + statOutput(df, testingAccMin, "%"));
	
			outputWriter("correctness on testing set: " + 
					"\n\t avg.: " + statOutput(df, testingCorAvg, "%") +
					"\n\t dev.: " + statOutput(df, testingCorDev, "%") +
					"\n\t max.: " + statOutput(df, testingCorMax, "%") + 
					"\n\t min.: " + statOutput(df, testingCorMin, "%"));
	
			outputWriter("completeness on testing set: " + 
					"\n\t avg.: " + statOutput(df, testingComAvg, "%") + 
					"\n\t dev.: " + statOutput(df, testingComDev, "%") +
					"\n\t max.: " + statOutput(df, testingComMax, "%") + 
					"\n\t min.: " + statOutput(df, testingComMin, "%"));
			
			outputWriter("FMesure on testing set: " + 
					"\n\t avg.: " + statOutput(df, testingFMesureAvg, "%") +
					"\n\t dev.: " + statOutput(df, testingFMesureDev, "%") +
					"\n\t max.: " + statOutput(df, testingFMesureMax, "%") +
					"\n\t min.: " + statOutput(df, testingFMesureMin, "%"));
		}
	}



	class URIComparator implements Comparator<Individual> {
		public int compare(Individual o1, Individual o2) {
			return o1.getURI().compareTo(o2.getURI());
		}
		
	}
	
	class CoverageComparator implements Comparator<CELOE.PartialDefinition> {
		public int compare(CELOE.PartialDefinition p1, CELOE.PartialDefinition p2) {
			if (p1.getCoverage() > p2.getCoverage())
				return -1;
			else if (p1.getCoverage() < p2.getCoverage())
				return 1;
			else
				return new ConceptComparator().compare(p1.getDescription(), p2.getDescription());
				
		}
	}
}
